const { OpenAI } = require('openai');
const fs = require('fs');
const path = require('path');
const prisma = require('../client');
const debugLogService = require('./debugLogService');
const { extractJsonFromString } = require('../utils/jsonUtils');
const { chunkTranscript } = require('../utils/chunkUtils');

const openrouter = new OpenAI({
  baseURL: "https://openrouter.ai/api/v1",
  apiKey: process.env.OPENROUTER_API_KEY,
});

const promptPath = path.join(__dirname, '../prompts/claim_extraction.prompt.txt');
const SYSTEM_PROMPT = fs.readFileSync(promptPath, 'utf-8');

/**
 * Extraction des claims par chunks avec orchestration de multiples appels LLM
 * @param {number} analysisId - L'ID de l'analyse (pour nommer les fichiers)
 * @param {object} structuredTranscript - La transcription structur√©e (content)
 * @param {string} model - Le nom du mod√®le LLM utilis√©
 */
async function extractClaimsWithTimestamps(analysisId, structuredTranscript, model) {
  console.log(`üîÑ D√©but de l'extraction par chunks avec le mod√®le: ${model}`);
  
  // 1. V√©rifier que nous avons des paragraphes
  if (!structuredTranscript.paragraphs || !Array.isArray(structuredTranscript.paragraphs) || structuredTranscript.paragraphs.length === 0) {
    console.error("Aucun paragraphe trouv√© dans la transcription structur√©e");
    return [];
  }

  // 2. R√©cup√©rer les param√®tres de chunking depuis l'environnement
  const chunkSize = parseInt(process.env.CHUNK_SIZE) || 4;
  const chunkOverlap = parseInt(process.env.CHUNK_OVERLAP) || 1;
  
  console.log(`üìä Configuration chunks: taille=${chunkSize}, chevauchement=${chunkOverlap}`);

  // 3. D√©couper la transcription en chunks
  const chunks = chunkTranscript(structuredTranscript.paragraphs, chunkSize, chunkOverlap);
  
  if (chunks.length === 0) {
    console.warn("Aucun chunk g√©n√©r√© √† partir de la transcription");
    return [];
  }

  console.log(`üì¶ ${chunks.length} chunks g√©n√©r√©s pour l'extraction`);

  // 4. Traiter les chunks par lots et collecter tous les claims
  const allClaims = [];
  const limit = parseInt(process.env.CONCURRENT_CHUNK_LIMIT) || 3;
  
  console.log(`üöÄ Traitement par lots avec une limite de ${limit} chunks simultan√©s`);

  // Cr√©er un tableau de t√¢ches (une t√¢che par chunk)
  const tasks = chunks.map(chunk => async () => {
    console.log(`üîç Traitement du chunk ${chunk.id} (${chunk.startTime}s - ${chunk.endTime}s)`);
    
    try {
      // Sauvegarder le prompt pour ce chunk
      const chunkSubfolder = `extraction/chunk_${chunk.id}`;
      debugLogService.log(
        analysisId,
        '1_prompt.txt',
        `--- PROMPT ENVOY√â AU MOD√àLE : ${model} ---\n--- CHUNK ${chunk.id} (${chunk.startTime}s - ${chunk.endTime}s) ---\n\n${chunk.text}`,
        chunkSubfolder
      );

      // Appel au LLM pour ce chunk
      const response = await openrouter.chat.completions.create({
        model: model,
        messages: [
          { role: "system", content: SYSTEM_PROMPT },
          { role: "user", content: chunk.text }
        ],
        response_format: { type: "json_object" },
      });

      const rawJson = response.choices[0].message.content;
      
      // Sauvegarder la r√©ponse brute
      debugLogService.log(
        analysisId,
        '2_response.txt',
        rawJson,
        chunkSubfolder
      );

      // Parser la r√©ponse JSON
      const cleanedJson = extractJsonFromString(rawJson);
      if (!cleanedJson) {
        throw new Error("Aucun objet JSON valide n'a pu √™tre extrait de la r√©ponse du LLM.");
      }

      const parsedData = JSON.parse(cleanedJson);
      if (!parsedData.claims || !Array.isArray(parsedData.claims)) {
        console.warn(`Chunk ${chunk.id}: Aucun claim trouv√© ou structure JSON invalide`);
        return [];
      }

      // Traiter les claims de ce chunk (sans raffinage, on fait confiance au LLM)
      const chunkClaims = parsedData.claims.map(claimData => {
        const { claim: claimText, estimated_timestamp } = claimData;
        
        // Validation basique du timestamp
        const timestamp = typeof estimated_timestamp === 'number' && estimated_timestamp >= 0
          ? estimated_timestamp
          : chunk.startTime; // Fallback au d√©but du chunk si timestamp invalide

        return {
          text: claimText,
          timestamp: timestamp
        };
      });

      console.log(`‚úÖ Chunk ${chunk.id}: ${chunkClaims.length} claims extraits`);
      return chunkClaims;

    } catch (error) {
      console.error(`‚ùå Erreur lors du traitement du chunk ${chunk.id}:`, error.message);
      
      // Sauvegarder l'erreur pour debug
      debugLogService.log(
        analysisId,
        '2_response_FAILED.txt',
        `ERREUR: ${error.message}`,
        `extraction/chunk_${chunk.id}`
      );
      
      // Retourner un tableau vide en cas d'erreur
      return [];
    }
  });

  // Traiter les t√¢ches par lots
  for (let i = 0; i < tasks.length; i += limit) {
    const batch = tasks.slice(i, i + limit);
    const batchNumber = Math.floor(i / limit) + 1;
    const totalBatches = Math.ceil(tasks.length / limit);
    
    console.log(`üì¶ Traitement du lot ${batchNumber}/${totalBatches} (${batch.length} chunks)`);
    
    try {
      const batchResults = await Promise.all(batch.map(task => task()));
      
      // Agr√©ger les r√©sultats de ce lot
      for (const chunkClaims of batchResults) {
        allClaims.push(...chunkClaims);
      }
      
      console.log(`‚úÖ Lot ${batchNumber} termin√©`);
      
    } catch (error) {
      console.error(`‚ùå Erreur lors du traitement du lot ${batchNumber}:`, error.message);
      // Continuer avec les autres lots m√™me en cas d'erreur
    }
  }

  console.log(`üìã Total avant d√©doublonnage: ${allClaims.length} claims`);

  // 5. D√©doublonnage des claims (√† cause du chevauchement)
  const uniqueClaims = deduplicateClaims(allClaims, chunks, structuredTranscript.paragraphs);
  
  console.log(`‚ú® Total apr√®s d√©doublonnage: ${uniqueClaims.length} claims uniques`);
  
  // 6. Sauvegarder un r√©sum√© de l'extraction
  const extractionSummary = {
    totalChunks: chunks.length,
    chunkSize: chunkSize,
    chunkOverlap: chunkOverlap,
    model: model,
    totalClaimsBeforeDedup: allClaims.length,
    totalClaimsAfterDedup: uniqueClaims.length,
    extractionTimestamp: new Date().toISOString(),
    chunks: chunks.map(chunk => ({
      id: chunk.id,
      startTime: chunk.startTime,
      endTime: chunk.endTime,
      claimsExtracted: allClaims.filter(claim => 
        claim.timestamp >= chunk.startTime && claim.timestamp <= chunk.endTime
      ).length
    }))
  };

  debugLogService.log(
    analysisId,
    'extraction_summary.json',
    JSON.stringify(extractionSummary, null, 2),
    'extraction'
  );

  return uniqueClaims;
}

/**
 * Filtre les claims pour supprimer les doublons g√©n√©r√©s par le chevauchement des chunks.
 * La strat√©gie consiste √† ignorer les claims trouv√©s dans la zone de chevauchement
 * de chaque chunk, sauf pour le tout premier chunk.
 * @param {Array} allClaims - Liste de tous les claims extraits de tous les chunks.
 * @param {Array} chunks - La liste des chunks g√©n√©r√©s par chunkTranscript.
 * @param {Array} paragraphs - La liste des paragraphes originaux d'AssemblyAI.
 * @returns {Array} Claims uniques apr√®s filtrage.
 */
function deduplicateClaims(allClaims, chunks, paragraphs) {
  if (chunks.length <= 1) {
    return allClaims; // Pas de chevauchement s'il n'y a qu'un chunk ou moins.
  }

  const chunkOverlapSize = parseInt(process.env.CHUNK_OVERLAP) || 1;
  const uniqueClaims = [];
  const duplicatesRemoved = { count: 0 }; // Utiliser un objet pour passer par r√©f√©rence

  // Trier les claims par timestamp pour un traitement ordonn√©.
  const sortedClaims = allClaims.sort((a, b) => a.timestamp - b.timestamp);

  for (const claim of sortedClaims) {
    // Trouver √† quel chunk ce claim appartient.
    const parentChunk = chunks.find(chunk => claim.timestamp >= chunk.startTime && claim.timestamp <= chunk.endTime);
    
    if (!parentChunk) {
      // Cas peu probable, mais on le garde par s√©curit√©.
      uniqueClaims.push(claim);
      continue;
    }

    // Le premier chunk (id: 0) est notre source de v√©rit√©, on garde tout.
    if (parentChunk.id === 0) {
      uniqueClaims.push(claim);
      continue;
    }

    // Pour les chunks suivants, on d√©termine la "vraie" date de d√©but (apr√®s l'overlap).
    const firstNonOverlapParagraphIndex = parentChunk.paragraphIndices.start + chunkOverlapSize;

    // Si l'index est valide dans la liste des paragraphes...
    if (firstNonOverlapParagraphIndex < paragraphs.length) {
      const trueStartTime = Math.round(paragraphs[firstNonOverlapParagraphIndex].start / 1000);

      // On ne garde le claim que s'il est dans la partie "nouvelle" du chunk.
      if (claim.timestamp >= trueStartTime) {
        uniqueClaims.push(claim);
      } else {
        // Ce claim est dans la zone de chevauchement, on le consid√®re comme un doublon.
        duplicatesRemoved.count++;
      }
    } else {
      // Ce chunk est enti√®rement compos√© de paragraphes de chevauchement (cas final), on ne garde rien.
      duplicatesRemoved.count++;
    }
  }

  if (duplicatesRemoved.count > 0) {
    console.log(`üîÑ ${duplicatesRemoved.count} doublons potentiels supprim√©s gr√¢ce au filtrage par chevauchement.`);
  }

  return uniqueClaims;
}

/**
 * MODIFICATION : Simule l'extraction des "claims" avec des timestamps.
 * @returns {Promise<Array<{text: string, timestamp: number}>>} Un tableau d'objets "claim" simul√©s.
 */
async function mockExtractClaimsFromText() {
  console.log('MOCK_CLAIM_EXTRACTOR: D√©marrage de l\'extraction simul√©e.');
  
  const defaultClaims = [
    { text: "Ceci est une premi√®re affirmation simul√©e.", timestamp: 10 },
    { text: "Une deuxi√®me affirmation de test est apparue.", timestamp: 25 },
    { text: "La simulation est un succ√®s.", timestamp: 42 }
  ];

  const existingClaims = await prisma.claim.findMany({
    take: 10,
    where: { timestamp: { gt: 0 } } // On prend des 'claims' qui ont un vrai timestamp
  });

  if (existingClaims.length < 3) {
    console.warn("MOCK_CLAIM_EXTRACTOR: Pas assez de 'claims' en BDD. Retourne des donn√©es par d√©faut.");
    return defaultClaims;
  }

  const shuffled = existingClaims.sort(() => 0.5 - Math.random());
  const selectedClaims = shuffled.slice(0, 3).map(c => ({ text: c.text, timestamp: c.timestamp }));
  
  console.log(`MOCK_CLAIM_EXTRACTOR: Utilisation de ${selectedClaims.length} 'claims' existants.`);
  await new Promise(resolve => setTimeout(resolve, 1500));

  console.log('MOCK_CLAIM_EXTRACTOR: ‚úÖ Extraction simul√©e termin√©e !');

  return selectedClaims;
}

module.exports = { extractClaimsWithTimestamps, mockExtractClaimsFromText };