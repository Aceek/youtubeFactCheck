const { OpenAI } = require('openai');
const fs = require('fs');
const path = require('path');
const prisma = require('../client');
const debugLogService = require('./debugLogService');
const { extractJsonFromString } = require('../utils/jsonUtils');
const { chunkTranscript } = require('../utils/chunkUtils');

const openrouter = new OpenAI({
  baseURL: "https://openrouter.ai/api/v1",
  apiKey: process.env.OPENROUTER_API_KEY,
});

const promptPath = path.join(__dirname, '../prompts/claim_extraction.prompt.txt');
const SYSTEM_PROMPT = fs.readFileSync(promptPath, 'utf-8');

/**
 * Fonction utilitaire pour effectuer un appel LLM avec syst√®me de retry
 * @param {Function} apiCall - Fonction qui effectue l'appel API
 * @param {number} maxRetries - Nombre maximum de tentatives
 * @param {number} delayMs - D√©lai entre les tentatives en millisecondes
 * @param {string} context - Contexte pour les logs (ex: "chunk 1")
 * @returns {Promise} R√©sultat de l'appel API
 */
async function callLLMWithRetry(apiCall, maxRetries, delayMs, context) {
  for (let attempt = 1; attempt <= maxRetries; attempt++) {
    try {
      const result = await apiCall();
      if (attempt > 1) {
        console.log(`‚úÖ ${context}: Succ√®s √† la tentative ${attempt}/${maxRetries}`);
      }
      return result;
    } catch (error) {
      console.error(`‚ùå ${context}: Erreur √† la tentative ${attempt}/${maxRetries}: ${error.message}`);
      
      if (attempt === maxRetries) {
        console.error(`üí• ${context}: √âchec d√©finitif apr√®s ${maxRetries} tentatives`);
        throw error;
      }
      
      console.log(`‚è≥ ${context}: Attente de ${delayMs}ms avant la tentative ${attempt + 1}...`);
      await new Promise(resolve => setTimeout(resolve, delayMs));
    }
  }
}

/**
 * Extraction des claims par chunks avec orchestration de multiples appels LLM
 * Maintenant avec sauvegarde progressive et syst√®me de retry
 * @param {number} analysisId - L'ID de l'analyse (pour nommer les fichiers)
 * @param {object} structuredTranscript - La transcription structur√©e (content)
 * @param {string} model - Le nom du mod√®le LLM utilis√©
 */
async function extractClaimsWithTimestamps(analysisId, structuredTranscript, model) {
  console.log(`üîÑ D√©but de l'extraction par chunks avec le mod√®le: ${model}`);
  
  // 1. V√©rifier que nous avons des paragraphes
  if (!structuredTranscript.paragraphs || !Array.isArray(structuredTranscript.paragraphs) || structuredTranscript.paragraphs.length === 0) {
    console.error("Aucun paragraphe trouv√© dans la transcription structur√©e");
    return [];
  }

  // 2. R√©cup√©rer les param√®tres de chunking depuis l'environnement
  const chunkSize = parseInt(process.env.CHUNK_SIZE) || 4;
  const chunkOverlap = parseInt(process.env.CHUNK_OVERLAP) || 1;
  
  console.log(`üìä Configuration chunks: taille=${chunkSize}, chevauchement=${chunkOverlap}`);

  // 3. D√©couper la transcription en chunks
  const chunks = chunkTranscript(structuredTranscript.paragraphs, chunkSize, chunkOverlap);
  
  if (chunks.length === 0) {
    console.warn("Aucun chunk g√©n√©r√© √† partir de la transcription");
    return [];
  }

  console.log(`üì¶ ${chunks.length} chunks g√©n√©r√©s pour l'extraction`);

  // 4. R√©cup√©rer les param√®tres de retry
  const maxRetries = parseInt(process.env.LLM_RETRY_COUNT) || 1;
  const retryDelay = parseInt(process.env.LLM_RETRY_DELAY_MS) || 2000;
  
  console.log(`üîÑ Configuration retry: ${maxRetries} tentatives max, d√©lai ${retryDelay}ms`);

  // 5. Traiter les chunks par lots avec sauvegarde progressive
  const limit = parseInt(process.env.CONCURRENT_CHUNK_LIMIT) || 3;
  
  console.log(`üöÄ Traitement par lots avec une limite de ${limit} chunks simultan√©s`);

  // Cr√©er un tableau de t√¢ches (une t√¢che par chunk)
  const tasks = chunks.map(chunk => async () => {
    console.log(`üîç Traitement du chunk ${chunk.id} (${chunk.startTime}s - ${chunk.endTime}s)`);
    
    const chunkSubfolder = `extraction/chunk_${chunk.id}`;
    
    // Sauvegarder le prompt pour ce chunk
    debugLogService.log(
      analysisId,
      '1_prompt.txt',
      `--- PROMPT ENVOY√â AU MOD√àLE : ${model} ---\n--- CHUNK ${chunk.id} (${chunk.startTime}s - ${chunk.endTime}s) ---\n\n${chunk.text}`,
      chunkSubfolder
    );

    // D√©finir la fonction d'appel API pour le retry
    const apiCall = async () => {
      const response = await openrouter.chat.completions.create({
        model: model,
        messages: [
          { role: "system", content: SYSTEM_PROMPT },
          { role: "user", content: chunk.text }
        ],
        response_format: { type: "json_object" },
      });

      const rawJson = response.choices[0].message.content;
      
      // Sauvegarder la r√©ponse brute
      debugLogService.log(
        analysisId,
        '2_response.txt',
        rawJson,
        chunkSubfolder
      );

      // Parser la r√©ponse JSON
      const cleanedJson = extractJsonFromString(rawJson);
      if (!cleanedJson) {
        throw new Error("Aucun objet JSON valide n'a pu √™tre extrait de la r√©ponse du LLM.");
      }

      const parsedData = JSON.parse(cleanedJson);
      if (!parsedData.claims || !Array.isArray(parsedData.claims)) {
        console.warn(`Chunk ${chunk.id}: Aucun claim trouv√© ou structure JSON invalide`);
        return [];
      }

      // Traiter les claims de ce chunk
      const chunkClaims = parsedData.claims.map(claimData => {
        const { claim: claimText, estimated_timestamp } = claimData;
        
        // Validation basique du timestamp
        const timestamp = typeof estimated_timestamp === 'number' && estimated_timestamp >= 0
          ? estimated_timestamp
          : chunk.startTime; // Fallback au d√©but du chunk si timestamp invalide

        return {
          text: claimText,
          timestamp: timestamp
        };
      });

      console.log(`‚úÖ Chunk ${chunk.id}: ${chunkClaims.length} claims extraits`);
      return chunkClaims;
    };

    try {
      // Appel avec syst√®me de retry
      return await callLLMWithRetry(
        apiCall,
        maxRetries,
        retryDelay,
        `Chunk ${chunk.id}`
      );
    } catch (error) {
      console.error(`üí• √âchec d√©finitif du chunk ${chunk.id} apr√®s ${maxRetries} tentatives:`, error.message);
      
      // Sauvegarder l'erreur pour debug
      debugLogService.log(
        analysisId,
        '2_response_FAILED.txt',
        `ERREUR D√âFINITIVE apr√®s ${maxRetries} tentatives: ${error.message}`,
        chunkSubfolder
      );
      
      // Retourner un tableau vide en cas d'√©chec d√©finitif
      return [];
    }
  });

  // 6. Traiter les t√¢ches par lots avec sauvegarde progressive
  const allClaims = [];
  let processedChunks = 0;
  
  for (let i = 0; i < tasks.length; i += limit) {
    const batch = tasks.slice(i, i + limit);
    const batchNumber = Math.floor(i / limit) + 1;
    const totalBatches = Math.ceil(tasks.length / limit);
    
    console.log(`üì¶ Traitement du lot ${batchNumber}/${totalBatches} (${batch.length} chunks)`);
    
    try {
      const batchResults = await Promise.all(batch.map(task => task()));
      
      // Agr√©ger les r√©sultats de ce lot
      const batchClaims = [];
      for (const chunkClaims of batchResults) {
        batchClaims.push(...chunkClaims);
        allClaims.push(...chunkClaims);
      }
      
      processedChunks += batch.length;
      
      // Calculer et mettre √† jour le progr√®s
      const progress = Math.round((processedChunks / chunks.length) * 100);
      
      console.log(`‚úÖ Lot ${batchNumber} termin√© - ${batchClaims.length} nouveaux claims`);
      console.log(`üìä Progr√®s: ${processedChunks}/${chunks.length} chunks (${progress}%)`);
      
      // SAUVEGARDE PROGRESSIVE : Sauvegarder les claims de ce lot imm√©diatement
      if (batchClaims.length > 0) {
        // D√©doublonner seulement les nouveaux claims de ce lot
        const uniqueBatchClaims = deduplicateNewClaims(batchClaims, allClaims.slice(0, allClaims.length - batchClaims.length));
        
        if (uniqueBatchClaims.length > 0) {
          await prisma.claim.createMany({
            data: uniqueBatchClaims.map(claim => ({
              analysisId: analysisId,
              text: claim.text,
              timestamp: claim.timestamp,
            })),
            skipDuplicates: true
          });
          
          console.log(`üíæ ${uniqueBatchClaims.length} nouveaux claims sauvegard√©s en base`);
        }
      }
      
      // Mettre √† jour le statut et le progr√®s de l'analyse
      await prisma.analysis.update({
        where: { id: analysisId },
        data: {
          status: processedChunks < chunks.length ? 'PARTIALLY_COMPLETE' : 'EXTRACTING_CLAIMS',
          progress: progress
        }
      });
      
    } catch (error) {
      console.error(`‚ùå Erreur lors du traitement du lot ${batchNumber}:`, error.message);
      // Continuer avec les autres lots m√™me en cas d'erreur
      processedChunks += batch.length; // Compter les chunks m√™me en cas d'erreur
    }
  }

  console.log(`üìã Total final: ${allClaims.length} claims extraits`);

  // 7. D√©doublonnage final (pour s'assurer de la coh√©rence)
  const uniqueClaims = deduplicateClaims(allClaims, chunks, structuredTranscript.paragraphs);
  
  console.log(`‚ú® Total apr√®s d√©doublonnage final: ${uniqueClaims.length} claims uniques`);
  
  // 8. Sauvegarder un r√©sum√© de l'extraction
  const extractionSummary = {
    totalChunks: chunks.length,
    chunkSize: chunkSize,
    chunkOverlap: chunkOverlap,
    model: model,
    maxRetries: maxRetries,
    retryDelay: retryDelay,
    totalClaimsBeforeDedup: allClaims.length,
    totalClaimsAfterDedup: uniqueClaims.length,
    extractionTimestamp: new Date().toISOString(),
    chunks: chunks.map(chunk => ({
      id: chunk.id,
      startTime: chunk.startTime,
      endTime: chunk.endTime,
      claimsExtracted: allClaims.filter(claim =>
        claim.timestamp >= chunk.startTime && claim.timestamp <= chunk.endTime
      ).length
    }))
  };

  debugLogService.log(
    analysisId,
    'extraction_summary.json',
    JSON.stringify(extractionSummary, null, 2),
    'extraction'
  );

  // Retourner tous les claims (ils sont d√©j√† en base gr√¢ce √† la sauvegarde progressive)
  return uniqueClaims;
}

/**
 * D√©doublonne les nouveaux claims d'un lot par rapport aux claims d√©j√† trait√©s
 * @param {Array} newClaims - Nouveaux claims √† v√©rifier
 * @param {Array} existingClaims - Claims d√©j√† trait√©s
 * @returns {Array} Claims uniques du nouveau lot
 */
function deduplicateNewClaims(newClaims, existingClaims) {
  if (!existingClaims || existingClaims.length === 0) {
    return newClaims;
  }
  
  const uniqueNewClaims = [];
  const threshold = 0.8; // Seuil de similarit√© pour consid√©rer deux claims comme identiques
  
  for (const newClaim of newClaims) {
    let isDuplicate = false;
    
    for (const existingClaim of existingClaims) {
      // V√©rifier la similarit√© textuelle et temporelle
      const textSimilarity = calculateTextSimilarity(newClaim.text, existingClaim.text);
      const timeDifference = Math.abs(newClaim.timestamp - existingClaim.timestamp);
      
      if (textSimilarity > threshold && timeDifference < 30) { // 30 secondes de tol√©rance
        isDuplicate = true;
        break;
      }
    }
    
    if (!isDuplicate) {
      uniqueNewClaims.push(newClaim);
    }
  }
  
  return uniqueNewClaims;
}

/**
 * Calcule la similarit√© entre deux textes (simple approximation)
 * @param {string} text1 - Premier texte
 * @param {string} text2 - Deuxi√®me texte
 * @returns {number} Score de similarit√© entre 0 et 1
 */
function calculateTextSimilarity(text1, text2) {
  if (!text1 || !text2) return 0;
  
  const words1 = text1.toLowerCase().split(/\s+/);
  const words2 = text2.toLowerCase().split(/\s+/);
  
  const set1 = new Set(words1);
  const set2 = new Set(words2);
  
  const intersection = new Set([...set1].filter(x => set2.has(x)));
  const union = new Set([...set1, ...set2]);
  
  return intersection.size / union.size;
}

/**
 * Filtre les claims pour supprimer les doublons g√©n√©r√©s par le chevauchement des chunks.
 * La strat√©gie consiste √† ignorer les claims trouv√©s dans la zone de chevauchement
 * de chaque chunk, sauf pour le tout premier chunk.
 * @param {Array} allClaims - Liste de tous les claims extraits de tous les chunks.
 * @param {Array} chunks - La liste des chunks g√©n√©r√©s par chunkTranscript.
 * @param {Array} paragraphs - La liste des paragraphes originaux d'AssemblyAI.
 * @returns {Array} Claims uniques apr√®s filtrage.
 */
function deduplicateClaims(allClaims, chunks, paragraphs) {
  if (chunks.length <= 1) {
    return allClaims; // Pas de chevauchement s'il n'y a qu'un chunk ou moins.
  }

  const chunkOverlapSize = parseInt(process.env.CHUNK_OVERLAP) || 1;
  const uniqueClaims = [];
  const duplicatesRemoved = { count: 0 }; // Utiliser un objet pour passer par r√©f√©rence

  // Trier les claims par timestamp pour un traitement ordonn√©.
  const sortedClaims = allClaims.sort((a, b) => a.timestamp - b.timestamp);

  for (const claim of sortedClaims) {
    // Trouver √† quel chunk ce claim appartient.
    const parentChunk = chunks.findLast(chunk => claim.timestamp >= chunk.startTime && claim.timestamp <= chunk.endTime);
    
    if (!parentChunk) {
      // Cas peu probable, mais on le garde par s√©curit√©.
      uniqueClaims.push(claim);
      continue;
    }

    // Le premier chunk (id: 0) est notre source de v√©rit√©, on garde tout.
    if (parentChunk.id === 0) {
      uniqueClaims.push(claim);
      continue;
    }

    // Pour les chunks suivants, on d√©termine la "vraie" date de d√©but (apr√®s l'overlap).
    const firstNonOverlapParagraphIndex = parentChunk.paragraphIndices.start + chunkOverlapSize;

    // Si l'index est valide dans la liste des paragraphes...
    if (firstNonOverlapParagraphIndex < paragraphs.length) {
      const trueStartTime = Math.round(paragraphs[firstNonOverlapParagraphIndex].start / 1000);

      // On ne garde le claim que s'il est dans la partie "nouvelle" du chunk.
      if (claim.timestamp >= trueStartTime) {
        uniqueClaims.push(claim);
      } else {
        // Ce claim est dans la zone de chevauchement, on le consid√®re comme un doublon.
        duplicatesRemoved.count++;
      }
    } else {
      // Ce chunk est enti√®rement compos√© de paragraphes de chevauchement (cas final), on ne garde rien.
      duplicatesRemoved.count++;
    }
  }

  if (duplicatesRemoved.count > 0) {
    console.log(`üîÑ ${duplicatesRemoved.count} doublons potentiels supprim√©s gr√¢ce au filtrage par chevauchement.`);
  }

  return uniqueClaims;
}

/**
 * MODIFICATION : Simule l'extraction des "claims" avec des timestamps.
 * @returns {Promise<Array<{text: string, timestamp: number}>>} Un tableau d'objets "claim" simul√©s.
 */
async function mockExtractClaimsFromText() {
  console.log('MOCK_CLAIM_EXTRACTOR: D√©marrage de l\'extraction simul√©e.');
  
  const defaultClaims = [
    { text: "Ceci est une premi√®re affirmation simul√©e.", timestamp: 10 },
    { text: "Une deuxi√®me affirmation de test est apparue.", timestamp: 25 },
    { text: "La simulation est un succ√®s.", timestamp: 42 }
  ];

  const existingClaims = await prisma.claim.findMany({
    take: 10,
    where: { timestamp: { gt: 0 } } // On prend des 'claims' qui ont un vrai timestamp
  });

  if (existingClaims.length < 3) {
    console.warn("MOCK_CLAIM_EXTRACTOR: Pas assez de 'claims' en BDD. Retourne des donn√©es par d√©faut.");
    return defaultClaims;
  }

  const shuffled = existingClaims.sort(() => 0.5 - Math.random());
  const selectedClaims = shuffled.slice(0, 3).map(c => ({ text: c.text, timestamp: c.timestamp }));
  
  console.log(`MOCK_CLAIM_EXTRACTOR: Utilisation de ${selectedClaims.length} 'claims' existants.`);
  await new Promise(resolve => setTimeout(resolve, 1500));

  console.log('MOCK_CLAIM_EXTRACTOR: ‚úÖ Extraction simul√©e termin√©e !');

  return selectedClaims;
}

module.exports = { extractClaimsWithTimestamps, mockExtractClaimsFromText };